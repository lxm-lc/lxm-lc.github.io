---
layout: article
title: 琐记 [又名：梁侪碎碎念]
tags: 学习 日记
aside:
  toc: true
sidebar:
  nav: layouts
---

# 2025

## 2025.9

### `20250915`
- 讲数据结构的是一位看起来很和善的女老师，她提及要重视基础，感觉没什么坏的
- 作为基本的代码学科，我又没什么OI基础，还是打算好好上课，然后课后不仅要**积极并尽量独立的做完作业**，还要偶尔**刷leetcode**，也算是多多掌握算法，对之后实习和保研（如果有的话）机试都有好处；哪怕不计这些，提高算法的思维也是不错的，就当是给之后的进阶打一个比较基础的代码功底了
- 感觉形体舞蹈会教我变得如何更加**优雅**，我亦都觉得学过之后自己更有气质且更自信多点，偶尔暗示自己要调整体态，也是一件非常不错的事情！我觉得这将大大提升我的精气神，也会给人以更**自信**的感觉，是很好的事情！
- 感觉视频多媒体检索绪论讲的没啥意思，只能说期待之后的授课与实践内容吧；但是已经找到人组成了三人的队伍了，对这个队伍的基本架构还是很满意的（啊啊啊队友我喜欢你们啊啊啊）
- 无人驾驶算法居然要去lbl上课，超远的，晚饭没得吃了嘻嘻不了一点呜呜呜`:cry:`但是也期待一个后续授课喔，不知道这样一门课对于丰富我的简历来说有没有帮助呢


### `20250916`
- 我要好好学大物，其中一个措施就是**不再记录好看但和考试无关的笔记**，我觉得这个时候矻矻记东西不如听课，把那些非常直觉化的东西都弄懂了先，并且立志要**按时复习和写课后作业**，还没上到的内容可以先预习，**别把作业留到ddl然后抄答案**
- 凸优化虽然是全英文上课，但是江老师的发音和声音都好好听，感觉坐在第一排听这个真的是如沐春风
- 无比感谢自己暑假刷了一些ML内容，也看了一些英语网课，否则感觉很难听懂今天的凸优化课
- 感觉还是要**好好回顾今天凸优化上课的内容**，最好能把上课内容用自己的话复述出来，这是我认为听非中文课很重要的一环
- 感觉晚上的语言数据科学尚且不错，有一点借助人工智能处理自然语言的感觉，亦有很多涉及语言本身的知识，和我预期的相符，可惜晚上事情太多，第二节课几乎没听
- 晚上搞完了**素拓上传**和社会实践课程的资料收集与上传，不能说效率有多高，但是**完成事情**总是令人无比开心、**有成就感**，感觉我上学期在素拓方面的表现不错，这学期再接再厉，争取大二结束能拿果酱（当然课内也要继续努力啊，hkpfs还是很需要这个的`:hard_working:`）
- 进组的事情还没找人聊过，不过后来想想本科生可能就是螺丝钉类型的角色，进哪个都无所谓了
<!-- - 所以最近打算找学长聊一聊，但是在此之前我还是要多了解一些**出国**的水源帖子，了解**暑研**基本过程，再去邀约他，才会显得真诚用心，具体问一些暑研的细则与申请的关注时间点，亦都要问他的导师是谁，是否比较推荐，他要是推荐的话我就直接考虑投简历然后进组了！加油！！！ -->

### `20250917`
- 今天上午上了**概统**，感觉老师讲的挺不错的，起码能把我原来不会（实则是已经遗忘了的）的东西讲明白，我感觉还是超级好的
- 虽然每节课要签到是坏的，但是鉴于我的数学自学实在是不容易开窍，往往需要到教室上课并在课后向老师询问我还没懂的点，所以这一点对我来说其实没什么损失就是了
- 总体而言，概统应该是后面很多AI相关的东西的理论基础之一，所以我还是励志要好好学这门课的（顺便也把我糟糕的**数学思维**补救一下了，感觉我数分除了积分那一块确实*曾经*会算，其他地方都是背证明过程、复刻证明思路了）
- 惊觉自己素拓有一部分传错位置了（妈耶，谁会知道点击不同学院这种连网址名都不会变的选项居然会影响审核啊😇），我最开始是想要等审核，审核不过就等第三轮上传；但是我不确定，就去把这个去问思政，他回复我说建议去办公室问一问；我一拍脑袋：啊这太对了，我一开始怎么想不到；后来去了办公室，老师建议我把之前的上传记录全删了重新传，我又感觉这太对了`:sob:`
- 然后我就开始反思为什么这么显而易见的事情我就想不到🤔后来一想，其实还是**经验不足**，如果我有类似的经验，我一定会第一时间删去重传；这件事情本质上不是“懂不懂这么做”，而是“能不能想到这么做”，中间可能差的不是诸如“**思想完备性**”云云，而是**有没有经历过**这件事
- 某种程度上，写代码/做工程可能就是这么会事；而在另一个维度上，这也启示了我们不要因为别人对某件事的不熟悉而感觉很奇怪乃至质疑，大家接触的东西不一样，**在未知的领域少一些认识是很自然的事情**，如果我们在这个时候**给予一些帮助与一定的科普**，其实能非常好的帮助对方，也创建一个更互惠互利的氛围，我觉得这是很好的
- 下午参与了Kimi圆桌月之暗面的活动，本来以为是前沿科研分享会，没想到是招聘……虽然有不少内容我无法听懂，但是感觉还是很有收获的！大概包括以下几点吧：
  1. 了解初创公司在做什么，目前的困难是什么样的
  2. 了解在其中实习需要的条件（力扣+研究经历+眼缘）
  3. 对人工智能的未来展开畅想，例如让AI帮我指定旅游计划，并为我购票
  感觉还是拓宽了我的眼界不少，也是为之后实习和就业做一些了解吧~
  *内心os：反正开学也没什么事情，参与一些科技性的活动也还是不错的*
<!-- - 其实我觉得最重要的是同一位学长的交流，一方面我明白了connection其实和老师是否资历多无关，主要还是看老师的博士是在哪里读的，另一方面，既然我之后想要去读PhD，也想真的体验科研，不妨就去找年轻的老师，可能刚入职三四年这样，他们会侧重于培养学生，也会为我提供一些帮助，我能与他们交流得到的反馈也更多，大抵也是能感受到学习和研究本身，而进大组可能就是流水线这样 -->
<!-- - 另一方面，进年轻导师的组可能就需要自己在科研上下功夫（毕竟没人会投喂成品论文），而且也要赌老师的人品和出路，但是我还是愿意一试的，因为我就是感觉自己气运不错，也愿意学，也敢沟通和提出问题，这样真的很好，真的很能发挥自己的能动性，一定会有收获的，更好的情况是也能获得很多很不错的成果！加油！为之努力吧！ -->
- 感觉近日要去官网上搜索一些导师资料，筹备进组了
- 晚上上了cv的课，因为对这个库不熟而导致上手有点困难，但是求助AI+自己理解之后感觉就好了不少，希望之后写这些作业不要太磕磕绊绊吧

### `20250918`
- 上午听大物感觉还不错，至少没有像上学期那样昏昏欲睡然后抬头起来再也听不懂了`:tieba_hehe:`
- 感觉这学期可以尝试**复述课堂内容以及思路**，感觉这样可以帮助我了解每个公式的意义 ~~从而记住公式本身并知道在什么时候可以用这个公式~~，啊啊啊一定要一雪前耻呜呜呜
- 找个时间把晚上的通识课退了，感觉根本没时间上课，不如好好自习（主要是时间碎片化太难受了，我真的想要**完整的学习时间**）
- 晚上发现志愿者活动的素拓全被驳回了，因为没有本人真名显示；想想这确实是合理的，但是哈基第二课堂把我的志愿活动全归于文体活动是什么鬼啊，我不理解，要不是因为怕这个影响判定，我也不会只放录取界面`:sob:`哈基电院 ~~哦不对，电院已经似了~~ 什么时候能第二课堂加一个“最后参与成功”的选项啊，这样之后全部系统指定，也没那么多奇奇怪怪的事情了`:cry:`
- 晚上和工试群u**散步**(●'◡'●) **大家都好可爱，喜欢大家喵~~**
- 我说散步是对的`:sob:`给我无聊的生活带来了无限乐趣
- 也总算是看完了西瓜书ML课程的倒数第二章了，只有聚类还没有学，感觉看完这些我或许不会在理论上有太多进步，但是对各个基本算法在干嘛应该有了不错的**了解**，之后应该一边**学习具体算法**，一边做**小项目**，做完了就可以**CS231n启动**了，争取（好吧，虽然这个不太可能，但是还是要给自己**打足信心**来码住哇！）在进组前基本上学完深度学习的课程内容，也相对应的做一些项目，感觉即使我之后不专做这方面的研究，了解一些**理论性的、基础性的**，对我也是**有好处**的
- 笑死了，为什么我这种在自习教室/图书馆安静环境下自制力存疑的人，在有室友说话的寝室里学习效率这么高，逆反心理了属于是（x）（虽然耳朵确实有点受罪（指把耳机摁进去听清楚点
- **学习日志**：看了ML课*7，并且复述了之前内容，感觉了解多一点了（实则是要是不复习就根本不记得了`:tieba_hehe:`）；趁午饭的时候看了Missing Semester半集 ~~真下饭~~，感觉其实学到了也不会用太多，更多的是了解性质的吧（说不定之后要用的时候就想起来我学过了呢（x
- **计划**：打算明天上午复习大物，然后去电院问素拓相关的事情，之后去上概统课程，下午就好好待在包图/自习教室复习、学新的东西，有想法的话可以开始去官网上搜老师的资料了，然后晚上和我的室友宝宝们吃饭饭，回来不知道要干什么，或许是娱乐吧
- 不过我感觉刷小视频娱乐/**长时间**看水源有点无趣且没有太多意义，刷太多学习/升学相关又容易焦虑而导致左耳朵进右耳朵出，打开某本社科论著开始看又容易有心理压力看不好，所以还是到时候随缘吧，实在不行水水CV报告也不是很坏了就是（x

### `20250919`
- ？怎么不在“志愿活动”下的志愿活动不能算在素拓里啊`:anon_angry:`哈基电你这家伙`:cry:`为什么去年（综测条例里这一项未修改）就能算是啊……我不懂啊，不要搞我啊呜呜呜，孩子当了那么久苦力就是想拿一点素拓，我也不容易啊`:sob:`（但是确实要好好看综测条例，不是吗，要是真搞不定我也认栽了`:tieba_hehe:`后编：这个确实没办法了，呜呜呜；后续：原来和组织单位有关，那没事了ww
- 上午的概统感觉听的半懂不懂，还是打算对着习题拟合一下
- 写了一些**文件管理经历与小tip**，结果午饭吃的特别晚，太坏了
- 下午斗志昂扬！打算开始搜集**导师信息**！
<!-- - 之前和学长聊过，刚刚又和已经进组的同学聊了一下，感觉我还是更倾向于比较年轻的导师，所以现在也尽量选年轻导师吧！也有听说CV/AI方向很火，比其他地方都好发论文，但是我感觉我对CV不是那么感兴趣，而且多少也有点畏惧竞争过于激烈；但也无所谓吧，万事还是开头最重要！加油哇！！！ -->
<!-- - 打算把导师信息搜集的内容放到md表格里，便于我后期对比 后编：失败力！要查的数据太多，谁爱干谁干吧-->
- 晚上和室友出去吃**海底捞**，嘻嘻嘻室友宝宝们我喜欢你们，你们都是最好的宝宝😘
- 查阅到比较中意的老师，打算<!-- 先做一些背调，然后 -->过两天写然后投个**简历**去问问

### `20250920`
- 周末还是太舒服了，一觉睡到了九点，然后去自习教室看西瓜书视频，终于看完啦！**完结撒花**喵~ *★,°*:.☆(￣▽￣)/$:*.°★* 
- 但是好饿好饿哦，到了饭点就即刻去吃饭，并打算下午先看一会儿Missing Semester作为消遣（x），然后开始写作业（哭）
- 感觉今天还是先学完概统，然后做大物作业吧，cv的报告糊一糊就可以上交啦
- 晚饭时间前后开了一个关乎CV课程大作业的**会**，大家进行了**问题的解构与分工**，感觉挺好
- 后续：下午回寝洗衣服，剩下的下午和晚上都在写概统作业，才干完了两次作业，效率幽默至极，但是较上学期有所提升
- 我觉得我需要培养**专注的能力**，不去看手机，专注写完作业，以及**调整作息保持学习高效**，并且要**把时间去碎片化**，在较长的时间里让自己进入心流状态，专注去完成一件事，我之后应该也会在随笔中提及这个

### `20250921`
- 作息良好，七点四十起床，上午洗衣服了一段时间，九点到自习教室，回顾大物内容并做作业，中午吃了轻食，下午一点半完成了本次作业
- 然后开始糊CV报告，感觉我来来回回花在这个上面的时间不下10h啊`:tieba_laugh:`这就是水报告的含金量嘛`:sob:`下次还是不要用这么多时间为好`:cry:`
- 晚上（应该说是深夜了）开始看ML的具体算法解析，打算开始**写小项目**力！
- 看了我之前下载的project-based-learning-for-ML仓库中Python模块下的[Write Linear Regression From Sctratch in Python](https://www.youtube.com/watch?v=uwwWVAgJBcM)，这个视频基于一个小的学习时间-成绩数据集手搓了一个**线性回归的Python代码**来探究两个变量之间的关系，作者讲的清楚明白，代码组织也非常inspiring（**先搭建基础框架，再对每一个函数进行实现**，过程相当流畅hhh）
- 感觉我从这个课程中获得的收获是：**线性回归的思路**是，总x-y-error的三维空间中，有一个向下凹的面，这个面是不同x y取值下，error的取值；因为是线性回归，所以总有唯一最低点，它旁边的点都高于它，再远一些的点可能趋于更大（甚至正无穷）；程序的核心意思是，当对每一个x y进行具体取值，都对应一条拟合直线，我们需要做的就是把原样本点和这条直线之间计算error，error越大则拟合效果越不好；我们要减小error，而且想要以最小的速度减小它，所以我们使用“**梯度**”这个概念，在函数的上该点(x, y)对x和y分别求偏微分，找到**下降最快的方向**；然后，在这个方向下，下降“**收敛速度**”个单位，注意，如果收敛速度太小，那么整个过程会非常慢，耗时长，而如果收敛速度过快，可能就划过了，难以**较为精准的降落error曲面的最低点**，所以“收敛速度”的取值是要有所权衡、多多尝试的；通过不断的下降，error不断的减少；最后，我们的x y将会接近那个让error取值最小的点，我们就获得了最终答案
- SO GREAT! 我已经把此次学习内容的整体思路梳理了一遍，相信我会记忆的更加深刻！同时，我也会感受到线性回归的一个主要思路是**不断修正现有取值，迫使其减小误差、高效的趋近最优解**
- 然后我又浏览了一遍[这位博主的主页](https://www.youtube.com/@SirajRaval/streams)，感觉比较多的都是零散的教程，例如在短时间内实现并讲解某小项目，单看起来还是很有意思的，但是不适合作为系统学习的资料；但是我作为一个初学者，在一开始就掌握所有内容是不现实的，我或许可以通过看这些小视频来**激发兴趣**，并且大多都当作**科普了解**，这都算是**相当好的学习体验了**`:love:`
<!-- - 我计划在明日上午再看几个这位博主的视频，尤其是其中关于CV图像识别的，这或许对我的CV大作业有所帮助 -->
- 不知不觉竟然在自习教室呆到23:40了，这相当晚了；或许我应该**早些回去睡觉**，然后明天保持**充足的精力与体力**，**继续学习**！（嘻嘻嘻好开心捏( •̀ ω •́ )y

### `20250922`
- 今日观看了昨日提及的博主讲到的**用OpenCV识别草莓**（主要是基于颜色检测识别），感受到OpenCV的**库好多**好全啊，但如果是我们做作业，甚至连有哪些库都不知道，自然没法形成完整的思路求解；而查阅文档或逐个搜索也只是能获得各个函数，而如果没有系统性的学习其思路，有函数也不会用；这是我们上视频多媒体这门课并完成作业的一个很大阻碍
- 今日观看了一个ML的初步教程[**iris的分类**](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)，主要思路是读入数据，对数据初步分析（计算平均值、方差、四分位数等）并可视化（箱型图、散点图等），然后使用多种模型，对其进行k-fold Cross Validation，最后互相比较成功率以及稳定性，选取了最优解（此处是SVM，也就是支持向量机），并用最优解训练模型，用到测试集上，表现出了较高的正确性；感觉这是一个不错的具体实践，主要通过调包方式展现了针对具体问题的机器学习代码实现，还是很不错的
- 看到一个模型保存的简单教程，其中提到了pickle和joblib这两种办法，感觉知识++了
  - **pickle**：python标准库，通用的序列化工具，可以保存、加载几乎所有python对象，但是处理大型的numpy数组的时候效率低下，储存的体积也比较大
  - **joblib**：sklearn官方推荐的工具，专门针对包含大规模numpy数组的对象优化过，也由此更适用于大规模数组（保存/加载更快，储存更高效），并且可以和joblib的`Paralled`做并行运算（把循环中的任务并行化，利用多核CPU同时处理，提升处理速度，在任务如训练多个模型、计算很多次交叉验证、数据预处理里的批量操作时可以使用）
- 凹糟的多媒体课，需要电脑但在中院，游戏本没有电源卡的要死，**哈基排课**你这家伙`:hand:``:sob:``:hand:`
- 下午在CV课上摸鱼写完了大作业的基本代码，还没开始训，但是训练模型这玩意要至少6个小时，运气不好的话可能要15+h，显然不能在我电脑快没电关机的时候跑(\*^_^\*) 家人们我破防了，现在已经只有17%的电量了，我将尝试在最后的关卡写一点文件管理的文章，然后就关机哩！
- 晚上上了小车的课，感觉这个软硬件结合还是比较麻烦的
- 晚上继续看了一个与分辨红酒品类好坏相关的project，但是鉴于时间问题，只看了数据统计的部分，真的感觉python自带的plt太神了`:exploded_head:`怎么能画出复合图🤣机器学习的部分看起来不多，我打算明天物理课下再看，感觉挺有意思的
- 以及联想到之前西瓜书里有说要**针对每个问题，选择适合它们的算法**；或许，用**数据统计**的方式可以**整体感知数据状态**，从而更方便我们判断机器学习用的方法等吧`:love:`
- 欸其实感觉保持现在这个状态刷项目（就是看看具体数据统计、机器学习代码实现等而并不直接攥写代码），我现在的库存应该会比较快就清空了欸；感觉到最后其实就是基本了解各个算法的代码实现思路，也不一定需要完全会写（AI会出手是这样的，我只要能看懂、有基本debug方向就基本能满足目前的需求）；此后就可以CS231n启动啦~还是很期待结合了project的、我之前未接触过的专业课程的:love:~~ ( •̀ ω •́ )y
- 深夜突然想起来如果准备赴港，应该要准备**语言成绩**啊`:nerd:`但是寒假考试又太早，所以最近还是有空就听英语资讯吧🐱（hhh暂时还是不是很焦虑这个，感觉这个确实最好刷的高一点才会利好奖学金的申请，但是最后分数高低又不是我今天焦虑明天焦虑能定的🤣所以还是平常心吧，平时学倦了可以听听一些英文音频什么的，或许会对我依托答辩的听力有所裨益耶


### `20250923`
- 上午听了大物课，感觉一些研究方法并没很懂，但是再说吧，下午多刷点题得了
- 早十没课，回到寝室继续看昨天跑的代码，才到42/100😅怎么能慢成这个鬼样子，而且跑出来效果也……不好评价，某图车旁边沾的广告被识别为车牌了，家人们谁懂这种无助感啊`:sob:`
- 中午在午饭的时候听了一集TED，听不懂一点，哭了；看来以后还要继续努力啊`:hard-working:`
- 下午上思修课，继续跑模型，继续写文件管理建议，继续看昨天葡萄酒分类的机器学习代码（后续：看完了），同时也开始准备写个人信息准备投递申请了（还是先想想我到底有什么履历可以写吧），同时也写一份面谈（如果有幸得到面试机会）的提问提纲（后续：提纲没时间写了）
- 感觉可以cs231n启动了，我还是比较期待能手搓项目的！
- 下午上了凸优化，发现自己已经完全不记得梯度的具体计算，只记得这是最速升降的方向了`:tieba_laugh:`
- 晚上去听了通识课，老师非常细致的讲了R语言的入门，我摸鱼一阵听一阵，最后看老师网站上的文档完成了学习
- 综合测评结果出来了，我看了一眼发现不管是我的成绩与排名还是我之前所预计的一些数据，其结果基本都在意料之中
- 上完课就开始摸鱼了，今晚23:45就睡了，感觉挺好，之后也要早睡，尽量在0:00之前，最好在23:30之前就上床睡觉，否则一天睡不到七个半小时还是有点难受的（更何况午觉可能是没有的）

### `20250924`
- 上午听了概统，感觉舒适，然后续了哈喽单车的卡，发现新卡没充进去是因为换了电话号码，这下我成纯小丑了🤡 （？才发现:clown_face:居然是`:clown_face:`而不是`:clown:`嘛？哈基小丑你这家伙！`:anon_angry:`）
- 回到寝室看了看我可爱的车牌识别模型，其测试结果令人哭笑不得，放几张图给大家看看

![为什么这些东西也会被识别为车牌啊，我不理解www](https://notes.sjtu.edu.cn/uploads/upload_6752ed7a36d7e285ce1d82a674e88897.png)

- 然后就来到了最令人期待的环节——cs231n，启动！
- cs231n的第一节到第三节都是将一些计算机视觉的背景以及本课程要求等，比较细节的知识是从第四课开始
- **第四课内容摘要**：
  - topic: 对图像进行分类
  - **Nearest Neighbor**
    - Step1: 给出一个测试集中的样本（在程序中表现为一个有很多维度的向量），将这个向量和一个`N × D`维度的向量做对比，这个向量是训练集中所有样本的表征，其中`N`代表样本总数，`D`代表每个样本的维度（在比较图片相似程度上，这可能是每个像素点对应的某项数值）；将这两个矩阵用numpy相减（numpy会自动复制`1 × D`的向量`N`次，从而变为`N × D`维，方便计算）
    - Step2: 将这个相减的矩阵呈现出来，并对每一行减出来的所有差作绝对值的总和（曼哈顿距离），选取距离最小的（也就是和新图最像的），把这个图的标签贴给新图，那么新图就被分类成功了
    - 缺点有：1. 训练模型只需要读取所有数据，很快，但是检测新样本需要很长时间（时间长度和样本总数成正比），效率不高；2. 容易误判（好比一大堆绿色的点中出现了一个黄点，那么新数据如果在黄点附近出现，就容易被判断为黄色，但实际上它可能就是绿色）
  - **K-Nearest Neighbors**
    - 为改进上面这一点，本方法找出了离新的样本点最近的k个点，并通过投票的方式确定这个样本点的类型；这样，当k>=3时，上文情况中新加的点也会被判定为绿色了（很有可能是噪点的黄色点对类别判定的影响就消失了）
    - k越大，各决策边界相较于Nearest Neighbor也更平滑好看；我们在实际操作的时候也倾向于给k赋一个比较大的值
  - **启发**：在计算机处理图像时，下面两种思维可以结合起来，在其中反复横跳，总能获得新的思维启发
    - **观点一：高维空间向量中的点（抽象）**  例如一张彩色图为`32 × 32`，其对应`D = 32 × 32 × 3`个值，那么这张图就对应一个`D`维空间中的点；通过**计算点与点之间的距离**，我们可以得到两张图的相似程度，这也就是Nearest Neighbor和K-Nearest Neighbors的基本思路
    - **观点二：观察具体图像（具象）**  虽然数学上图像是一个向量，但它在视觉上是一个有意义的、结构化的实体——它有边缘、纹理、形状和语义；例如，一只猫平移了，它的向量点和原先相比可能偏差了很多，但是它们本质上都是猫；这也就解释了上述两种算法的局限性，引出了下文要讲的其他算法
- **第五课内容摘要**：
  - L1距离（曼哈顿距离）和L2距离（欧式距离）在二维图形中分别表现为菱形与圆形；而且有意思的是，**选取不同坐标轴，两点之间的L2距离都是一样的，但是L1距离不一样**，这就启发了我们，如果一堆向量中**有几个特别重要的向量，则可以考虑使用L1距离**，否则，如果只是通用向量（不知道其具体含义），则L2可能更合适一些
  - **决策边界**可能受到**k值**和**距离度量方式**的影响；k值对其影响已经提及，而在距离度量上，使用L2就比用L1流畅自然许多，因为L2就是自然划分，而L1与坐标轴的选取有关（一些较远的分割线平行于x或y轴）
  - **超参数（hyperparameter）**是与算法选择相关，和机器自动选择的相比，这个更和人的选择息息相关
  - **如何设置超参数**：
    - 方法1：将数据划分为不相交的训练集、验证集、测试集，用不同超参数训练训练集，把各种模型用到验证集上，选出性能最好的几个模型，最后再用到测试集上；这里的**测试集**对于模型而言是完全未知的，而模型本身**正是要在完全没见过的数据集上表现良好**
    - 方法2：（常用于小的训练集）**交叉验证**，分所有数据为测试集和验证集，测试集分为好多小板块，每次把大多数板块用作训练，小部分板块用作验证
- **第六课内容摘要**：
  - **Linear Classifier（线性分类器）**：f(x, W) = Wx + b，每个类别学习单一模板；坏处是可能出现一些情况（例如蓝色在一三象限，红色在二四象限），没法用线来划分；在多分类问题中也是如此（比如马匹的照片可能头朝左，可能朝右，没法总结出定性的模型）；我询问了AI，得到了线性分类器几个主要的有点和缺点，如下：
    1. **运算速度块且高效**：许多线性模型具有封闭形式解或使用高效的凸优化算法（例如梯度下降），它们的训练时间常比复杂模型（如神经网络）少很多；预测阶段仅包含简单的向量点积运算，计算量极低，可用于即时性强的应用场景
    2. **模型可解释性强**：W包含了所有变量的权重，清晰明了，可以看得出哪些变量对我们的结果正相关/负相关/相关程度如何
    3. 在高维稀疏数组中表现良好，且实现代码简单，易于维护
    4. **无法处理非线性可分数据**：如同心圆分布、抑或分布，线性分类器无法找到合适的超片面进行分类
    5. **泛化能力受限**：它假设数据可以通过一条简单的线分割，在处理图像、语音等复杂、高变异数据时，性能上限较低
    6. **易受离群值影响**
- 惊觉最近几天除了上课和写作业之外都没碰过课内，还是不太好的，感觉平时还是要注意一下，以免到期末发现基础的都不会从而抱不了佛脚
- 晚上肝了概统的作业
##### 今日进度：cs231n-class (6/33) cs231n-project (0/3)


### `20250925`
- 家人们，**Gemini**还是太会大物了，我把我课上没有听懂的部分的PPT投喂给Gemini，它就能很清楚的把我讲懂，强推（我说免费而擅长多模态的模型是无敌的，有没有懂的`:hand:``:sob:``:hand:`）
- 凸优化强度也太高了吧，今天花了整整一个半小时把上节课的东西全部喂给AI理解了
- 虽然我并没能全部明白，但是今天复习到的凸优化大致讲到了梯度的概念，以及说对向量值函数，微分是梯度的转置（一个是行向量，另一个是列向量），并涉及各种函数的求导
- **第七课内容摘要**：图像分类-线性分类
  - 本节课主要提出了一种给图像分类的方法，例如，所有图片一共分为十个类，每张图片对应一个向量`x`；我们需要找到一个矩阵`W`，让`W × x`的结果在这张图片类别上的评分最高，而其他评分越低越好，这样就是比较好的分类；而针对这种方法，我们有两种测评损失的方法：
    1. 设定一个**阈值**，我们希望其他属性上的数值比正确属性上的数值小的程度超过阈值：如果能够做到，则没有损失；如果无法做到，则损失随着两者相差的减小（或反向相差的增大）而线性增大；最后，我们通过各种方法减小损失并鼓励选用不那么复杂的模型
    2. 把每个评分**正则化**：本质思想是`e^{单项评分}`，把这些数值相加，得到总和，再用每个计算得来的小数值除以总和，得到权重；我们希望正确的属性对应的权重最大，其他属性对应的权重最小
    - 这两种方法的不同之处是，对于基于阈值的方法，只要正确类的评分大于错误类的评分的程度多于一个数值，损失就是零，模型达到这一个目的就不会再倾向于让正确类的评分继续上升、错误类的评分继续下降；而对于使用正则化的方法，正确类的评分到1、错误类的评分到0是不可能出现的事情（这将要求原正确类评分趋向正无穷，错误类为负无穷），这将push模型不断增加正确类的评分和降低错误类的评分；有趣的是，**两者的实际训练效果相近**
  - 两种梯度计算方式：
    - **数值梯度**：使用有限差分法，计算量大，但是结果可靠，往往适用于验证
    - **解析梯度**：基于微积分得到梯度，计算量远远小于数值梯度，但是容易在编码的时候引入错误，所以往往需要数值梯度作为验证
    - 梯度是前进的方向，**步长**是学习速率，是非常重要的超参数
    - 随机梯度算法：因为向量总维数N可能很大，全都计算梯度可能会非常耗时，所以实际操作中可能会随机选取一些minibatch数据（32、64、128个等）
- ？？？神人排名被卡5/49在小专业10%外`:yaoming:`**专业综测第一没国奖是吧，IEEE奖学金割让给平台是吧，强卡小专业前10%是吧，我真是服了**，孩子破防了喵，骑车去校外逛了一圈并哭哭喵
##### 今日进度：cs231n-class (7/33) cs231n-project (0/3)


### `20250926`
- 今早与处境相同的学长找计院学生口argue了，反正结果就是按小专业排，改不了，详情见[水源投喂可爱IEEE楼相关楼层](https://shuiyuan.sjtu.edu.cn/t/topic/332600/886)`:tieba_hehe:`
- 多说无益了罢，不如好好学习，争取拿国奖+积攒很好的科研经验+发顶会，这样港府奖学金还是有可能的💪加油喵！
- 皮革兔了酸奶、奶茶、甜筒、鸭锁骨、鸭舌和怡泉+C，感觉整个人都从意志消沉变好起来了呢！
- 晚上平复心情，感觉还是要把**main focus**放在**学习**和**高质量爱好**上，前者总归能让我获得更多对生活和未来的**掌控权**，后者能让我充实心灵，说不定还有意外收获
- 那就先从**重拾社科文章阅读习惯**开始吧……上次那篇文章我看了1/4，然后好几个月没读了，现在估计要从头再来，就当是提升英文阅读水平了吧！
- **第八课内容摘要**：介绍神经网络-反向传播
  - 感觉没什么特殊的，就是为了对最终表达式求一个变量的偏导，但是直接计算数学表达式太累了，不妨先正向求值完毕，再反向从1开始反推各项导数，从而简化计算
- 明天要听讲座啦，晚上当苦力，再见这个学院不知道该说些什么呢`:tieba_hehe:`
- 还是趁国庆好好听cs231n吧，听得差不多了就投递简历进组啦~
##### 今日进度：cs231n-class (8/33) cs231n-project (0/3)


### `20250927`
- 上午提交了**三好学生报名申请**表，然后看了一遍**地铁志愿者**的资料并完成了测试，之后进行继续**填词**并填完
- 中午又吃了超模厨房的外卖，感觉这已经是减肥餐中相当好吃的了；同时看了cs231n，感觉这一课也没什么内容x
- **第九课内容摘要**：介绍神经网络-反向传播，通过加入非线性函数（例如`max(a, b)`），在线形层之间加入非线性层，从而让整个模型更加复杂、能作更高级的分辨（比如：头朝左、右的马均能被识别为马）
- **第十课内容摘要**：卷积神经网络的实现思路和神经网络类似，区别在于需要训练卷积层（这能更好的保留输入的空间结构）
  - 本课展现了卷积神经网络的历史和一些具体应用，给我留下比较深刻的印象的有两点
    1. 用卷积神经网络识别物体，未必要是用框框框住，而是可以识别一**块**的面积，用边界为曲线的图形包裹住它予以呈现
    2. 输入图片，输出对图片的描述
- 今天下午有讲座，晚上还有志愿活动，明早还有大物早课；或许我应该今日暂停听cs231n，先把大物补了，明天就能继续跟进度线下听课了
- cs231n已经布置了第一个作业久矣，但是最近没空，我打算在十月一号完成它；之后的任务也尽量在国庆假期完成，不给自己各种忙而拖延的借口
- 下午听讲座感觉**受益颇深**，尤其是和学长学姐（哎怎么还真有学姐啊😘这在你交可不常见🤣）的对话让我感觉挺有启发的，并且给我了新思路
- 如果是在一年前，我肯定会超级纠结然后权衡利弊，最后再做很艰难的选择，选完了若不够完美又要深深内耗；但是我现在已经改变，我大概不那么内耗了，只当每个流程是自然程序（或者说是流动的生命过程），我很清楚我做每个选择背后有哪些能让我成功做这个选择的筹码，也知道每个选择的试错成本，我知道我应该怎么讲话，以做一个我当下相对满意的抉择，或者是一个有益的探索——所以，勇敢行动起来吧！或也不必大谈何故如此，只当是个人第六感所牵引，把适当的事理以及思绪安置在适当的位置，至于各流程成败，自有天意相随
##### 今日进度：cs231n-class (10/33) cs231n-project (0/3)


### `20250928`
- 家人们，我怎么发现我看的是2017年的版本啊😱这下纯小丑了🤡不过也没事，我把目前的板块看完就无缝衔接到最新版好啦，感觉最新版对现在的版本只增不减
- **第十一课内容摘要**：卷积神经网络-卷积和池化
  - 首先，课程提及了全连接层和卷积层工作原理的不同：**全连接层**把所有像素以及其属性铺成一个一维向量，而**卷积层**可以维持原本的空间结构，仍旧可以保持三维的构型；后者的工作原理是，选取一块小面积（长宽往往非常小，厚度也即属性个数和训练数据相同）也就是**滤波器**，滤波器中含有某种特质（如纹理、颜色等），我们将其在原矩阵上平移（如原图是32×32×3，我们选择5×5×3的小块），每次匹配到就展开为一维向量，然后做点积，点积数值越大证明两者越接近（比如都是蓝色的边界），如果点积较小说明两者接近程度低，点积为负说明两者的特征相反
  - 我们可以挑选**步长**；一般而言，步长$S=1$时，可以保持特征图尺寸，专注于提取特征，而步长$S>1$，可以对特征进行降维，从而获得更大的感受野，对图像的总体特性做分析，也降低了分辨率，减少了计算量（取$S=2$时，感受野呈指数级增长）；同时，步长需要和原图匹配，如果出现不匹配的情况，需要特殊处理（例如在图像上使用零填充（zero padding）加边）
  - 各项参数：
    - Input: $W$
    - Filter: $K$
    - Padding: $P$
    - Stride: $S$
    - Output: $\frac{W-K+2P}{S}+1$
  - **池化**：缩小输入的规模，比如把得到的图形以2×2的不重叠格子划分，并从2×2矩阵中选取最大的数值填充进新图，即以2的倍数对所有东西进行**下采样**
- **终于确定了想要进的组！**这几天全部**满课**，写东西估计不显示😇等国庆期间再写个人简历，然后就可以投递啦~😘
- 思修上写完了大物预习报告，希望Gemini给我的总结归纳是对的吧；然后爽爽睡觉，家人们谁懂啊，爽爽睡觉还是太舒服了🐖
- 凸优化课很认真的听了喵，感觉这课是我目前少有的很感兴趣的课(❤ ω ❤)
- 我已经学会在通识课吃喝x（大雾）
- 不是哥们，20min的下课时间在干嘛啊😅晚饭也没得吃，在课上吃我不好意思老师也不太愿意，那为什么不考虑延后晚课时间呢😫
- 在课上爽刷水源，然后自学了数据结构，一看题目，呀，怎么不会呀😇感觉数组这块我是真不会吧，看来还要琢磨琢磨喵，先熟练吧😀
- 好晕啊，感觉这几天睡眠严重不足，各种减肥导致我精神状态也很坏，刚好遇到奖学金事变和经期，真的是debuff叠满了😅
- 这几天一定要**好好睡觉**、**多吃点好的东西**、**调整心态继续努力**喵！相信我一定会否极泰来的喵！❤
##### 今日进度：cs231n-class (5/18) cs231n-project (0/3)


### `20250929`
- **第六课摘要**：CNN架构
  - **Layer Normalization** (层归一化)：针对每一个样本，对其所有特征维度进行归一化（例如，一个盘子是一个样本，盘子上的每一种事物都是一种特征，层归一化就类似于修改你自己盘子里的每一种食物的含量，使其饱腹感都一样，而别人的盘子与你无关）；不依赖批量大小$N$
  - **Batch Normalization** (批归一化)：对于一个批次中所有样本的某一特性进行归一化（类比为确保每一个客人的**一种特定食物**饱腹感相同）；这需要基于一个足够大的$N$才能计算出可靠的统计量
  - **正则化方法Dropout**：通过随机“关闭”部分神经元，使得各元素之间强依赖性的可能性降低（例如，不管模型对属性A如何处理，都会有属性B为它兜底，这样即使乱给A的判定，对训练集结果都不错，但是当遇到陌生的数据时，A可能有其他表示，原本的模型可能不再奏效；而使用了dropout之后，模型必须提升鲁棒性才能更好的应对神经元的随机丢弃），从而能够提升模型的鲁棒性，增加了泛化能力（在测试阶段不丢弃任何神经元，从而模型能在测试集上性能更好）
- 睡足了七个小时；早饭和午饭都有好好吃，没有在减肥喵；和源友聊的很开心qwq 啊啊啊好开心o(*￣▽￣*)ブ喜欢捏
- 补了一些物理课，感觉对电势那一堆知识++（我看是高中物理全忘光了吧`:tieba_hehe:`），作业或许可以做了，但是哥们真有时间做吗`:tieba_happy:`（我还挺想好好做然后校对答案的，这学期物理一定要拿高分啊`:sob:`）
- 神人大物实验`12:30`开始，差点错过，幸亏伟大的群u告诉我正确时间了🐱
- CV课认真刷了cs231n，但是为什么九十分钟就看了20min啊`:tieba_hehe:`还是太求真务实了，听不懂的一律Gemini喂进`:huanhu:`我说多模态是对的（但是听英语课不带耳机是硬伤，AI翻译的中文可读性基本可视作零，有一种无效中文、文字进不了脑子的感觉）
- 小车课调AI神奇捏，随机掉线的热点痛苦捏~
- 啊上周忘记写周记了，要尽早补上并且推到网页端！
- 无人驾驶算法开发课还是学到一些的（实则用已有的代码让小车跑起来了，感觉还是很开心的，之后有空去读一下那个代码）
##### 今日进度：cs231n-class (6/18) cs231n-project (0/3)

### `20250930`
- 做了一天大雾作业，进度迟缓，难题不会，但是已经比上学期好很多惹，aaa梁侪猫猫要继续努力喔！
- 凸优化课有好多听不懂啊，看来只能课后诉诸Gemini了
- 晚上的通识课补了一整节课的大物😇也是太有生活了
##### 今日进度：cs231n-class (6/18) cs231n-project (0/3)



## 2025.10

### `20251001`
- **喜迎国庆**！我将在国庆期间把之前落下的**学习进度**一概补齐，并争取多多学习**课外知识**并**投递进组简历**（正如我前文说过，我将在十月上旬做这件事，没想到之前那么心急，竟然最后也都到了这个时候才开始做hhhh 或许也是有天意吧，我亦都要好好抓住这次机会！`:hard_working:`）
- 上午一觉睡到了十一点（）也是非常🐖了hhhh **补充精神，恢复精力**是也！
